{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The current working directory needs to be in explainable_TGCNN\n",
    "print(os.getcwd())\n",
    "os.chdir('..\\\\')\n",
    "print(os.getcwd())\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from src import create_fake_patients, whole_model_demographics_gradcam, graph_plot, ga, gc, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_visit_number(original_list):\n",
    "    original_versions = []\n",
    "    for tpl in original_list:\n",
    "        for item in tpl:\n",
    "            version = item.split('_')[1]  # Extract the version number part (e.g., 'v91')\n",
    "            original_versions.append(version)\n",
    "\n",
    "    # Create a mapping from original version numbers to new version numbers starting from 1\n",
    "    unique_versions = sorted(set(original_versions), key=lambda x: int(x[1:]))\n",
    "    version_mapping = {v: f\"v{i+1}\" for i, v in enumerate(unique_versions)}\n",
    "\n",
    "    # Replace the version numbers in the original tuples\n",
    "    new_list = [\n",
    "        (\n",
    "            f\"{item1.split('_')[0]}_{version_mapping[item1.split('_')[1]]}\",\n",
    "            f\"{item2.split('_')[0]}_{version_mapping[item2.split('_')[1]]}\"\n",
    "        )\n",
    "        for item1, item2 in original_list\n",
    "    ]\n",
    "    return new_list\n",
    "\n",
    "def add_tuples_to_dict(tuples_list, subgraph_dict, predicted_outcome):\n",
    "    # Sort each tuple within the list to ensure consistent ordering\n",
    "    sorted_tuples = tuple(sorted(tuples_list)) \n",
    "    \n",
    "    if sorted_tuples in subgraph_dict:\n",
    "        # If a subgraph exists, update the appropriate count\n",
    "        if predicted_outcome > 0.5:\n",
    "            subgraph_dict[sorted_tuples]['s+'] += 1\n",
    "        else:\n",
    "            subgraph_dict[sorted_tuples]['s-'] += 1\n",
    "    else:\n",
    "        # If a subgraph doesn't exist, add a new entry with a count of 1 in the appropriate column\n",
    "        subgraph_dict[sorted_tuples] = {'s+': 1 if predicted_outcome > 0.5 else 0,\n",
    "                                        's-': 1 if predicted_outcome <= 0.5 else 0}\n",
    "    \n",
    "    return subgraph_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_TGCNN_layer = True\n",
    "demo = True\n",
    "\n",
    "include_drugs = True\n",
    "max_timesteps=100\n",
    "\n",
    "stride = 1\n",
    "filter_size = 4\n",
    "num_filters=16\n",
    "\n",
    "run_name='hip_1999_to_one_year_advance_model'\n",
    "years_in_advance = \"5\"\n",
    "\n",
    "if include_drugs:\n",
    "    max_event_codes = 518\n",
    "else:\n",
    "    max_event_codes = 512\n",
    "hip_or_knee = 'hip'\n",
    "\n",
    "# fake mapping dataframe for the ReadCodes and the corresponding descriptions\n",
    "read_code_map_df = pd.read_csv('fake_read_code_descriptions.csv')\n",
    "\n",
    "model = whole_model_demographics_gradcam.TGCNN_Model(num_filters=16, num_nodes=max_event_codes, num_time_steps=max_timesteps, \n",
    "                            filter_size=filter_size, variable_gamma=True, \n",
    "                            exponential_scaling=True, dropout_rate=0.7, lstm_units=64,\n",
    "                            fcl1_units=128, LSTM_ablation=False, stride=stride, activation_type='LeakyReLU', \n",
    "                            no_timestamp=False, second_TGCNN_layer=second_TGCNN_layer, num_labels=1)\n",
    "model.load_weights('hip_1999_to_one_year_advance_model1_CNN_layer')\n",
    "\n",
    "# Load in the filters from the model\n",
    "with open(f'hip_1999_to_one_year_advance_model1_filter.npy', 'rb') as f:\n",
    "    filters = np.load(f)\n",
    "filters = tf.cast(filters, dtype=tf.float16)\n",
    "\n",
    "num_pats = 5\n",
    "max_act_filt_num = 10\n",
    "cv_patients = create_fake_patients.create_fake_patient_df(num_pats, 99, max_event_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph_dict = dict()\n",
    "\n",
    "for pat in range(num_pats):\n",
    "    \n",
    "    filt_type = 'median' # 'mean', 'median', 'max'\n",
    "    input_3d, input_4d, demo_tensor, outcome, outcome_bin = utils.return_pat_from_df(cv_patients, max_event_codes, hip_or_knee, pat, max_timesteps)\n",
    "    dense_tensor = tf.sparse.to_dense(input_3d)\n",
    "    dense_tensor= tf.transpose(dense_tensor, perm=[2, 1, 0])\n",
    "    dense_tensor = np.flip(dense_tensor, axis=0) # change the most recent events to be at the end rather than the start\n",
    "    dense_tensor = tf.cast(dense_tensor, tf.float16)\n",
    "    filters_4d = ga.make_filts_4d(filters, filter_size, max_event_codes)\n",
    "    \n",
    "    f = ga.get_and_reshape_filt(filters_4d, 30, filt_type=filt_type)\n",
    "    edge_act_graph = ga.filt_times_pat(f, dense_tensor, filter_size, max_timesteps, stride)\n",
    "    edges_df = ga.create_edges_df_ga(dense_tensor, edge_act_graph) \n",
    "    \n",
    "    # Get the node positions for the graph\n",
    "    pos_df = graph_plot.create_position_df_gc(edges_df)\n",
    "    pos_list = graph_plot.generate_pos_sequence(pos_df['max_codes_per_visit'].max())\n",
    "    pos_df = graph_plot.map_y_coord_to_node(pos_df, pos_list)\n",
    "    \n",
    "    read_code_pos_df = ga.map_read_code_labels(pos_df, read_code_map_df)\n",
    "    \n",
    "    edge_pos_df = ga.create_edge_pos_df(edges_df, pos_df)\n",
    "\n",
    "    # Remove repeat rows where edge_weight_perc == 0\n",
    "    mask = edge_pos_df['edge_weight_perc'] != edge_pos_df['edge_weight_perc'].shift()\n",
    "    df_unique_adjacent = edge_pos_df[mask].reset_index(drop=True)\n",
    "    \n",
    "    # Find indices where the edge_weight_perc col has a value of 0\n",
    "    split_indices = df_unique_adjacent.index[df_unique_adjacent['edge_weight_perc'] == 0].tolist()\n",
    "    \n",
    "    # Add start and end indices to make it easier to split\n",
    "    split_indices = [-1] + split_indices + [len(df_unique_adjacent)]\n",
    "    \n",
    "    # Split the DataFrame into chunks\n",
    "    chunks = [df_unique_adjacent.iloc[split_indices[i]+1:split_indices[i+1]] for i in range(len(split_indices)-1)]\n",
    "\n",
    "    logits = model(input_4d, demo_tensor, training=False)\n",
    "    proba = tf.sigmoid(logits)\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if len(chunk) != 0:\n",
    "            subgraph_list = list(zip(chunk['start_node'], chunk['end_node']))\n",
    "            subgraph_list_adj_vis = replace_visit_number(subgraph_list)\n",
    "            add_tuples_to_dict(subgraph_list_adj_vis, subgraph_dict, proba)\n",
    "\n",
    "    if (pat % 100) == 0 and (pat !=0):\n",
    "        print(f\"Number of patients complete: {pat}\")\n",
    "        print(f\"{(((pat+1)/num_pats)*100):.2f}% Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the dictionary and create a list of tuples\n",
    "data = []\n",
    "for subgraph, counts in subgraph_dict.items():\n",
    "    data.append({\n",
    "        'subgraph': subgraph,\n",
    "        's+': counts['s+'],\n",
    "        's-': counts['s-']\n",
    "    })\n",
    "\n",
    "# Convert the list of tuples into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Get the ratio of the subgraphs for each class\n",
    "df['total'] = df['s+'] + df['s-']\n",
    "df['Rs+'] = df['s+'] / df['total'].replace(0, np.nan)  # Avoid division by zero\n",
    "df['Rs-'] = df['s-'] / df['total'].replace(0, np.nan) \n",
    "\n",
    "df.drop(columns=['total'], inplace=True)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_msk_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
