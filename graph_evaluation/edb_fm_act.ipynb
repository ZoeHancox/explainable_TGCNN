{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The current working directory needs to be in explainable_TGCNN\n",
    "print(os.getcwd())\n",
    "os.chdir('..\\\\')\n",
    "print(os.getcwd())\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from src import create_fake_patients, whole_model_demographics_gradcam, graph_plot, plot_feature_value, ga, gc, utils\n",
    "import statistics\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_TGCNN_layer = True\n",
    "demo = True\n",
    "\n",
    "include_drugs = True\n",
    "max_timesteps=100\n",
    "\n",
    "stride = 1\n",
    "filter_size = 4\n",
    "\n",
    "run_name='hip_1999_to_one_year_advance_model'\n",
    "years_in_advance = \"5\"\n",
    "\n",
    "if include_drugs:\n",
    "    max_event_codes = 518\n",
    "else:\n",
    "    max_event_codes = 512\n",
    "hip_or_knee = 'hip'\n",
    "num_filters = 16\n",
    "\n",
    "# fake mapping dataframe for the ReadCodes and the corresponding descriptions\n",
    "read_code_map_df = pd.read_csv('fake_read_code_descriptions.csv')\n",
    "\n",
    "model = whole_model_demographics_gradcam.TGCNN_Model(num_filters=16, num_nodes=max_event_codes, num_time_steps=max_timesteps, \n",
    "                            filter_size=filter_size, variable_gamma=True, \n",
    "                            exponential_scaling=True, dropout_rate=0.7, lstm_units=64,\n",
    "                            fcl1_units=128, LSTM_ablation=False, stride=stride, activation_type='LeakyReLU', \n",
    "                            no_timestamp=False, second_TGCNN_layer=second_TGCNN_layer, num_labels=1)\n",
    "model.load_weights('hip_1999_to_one_year_advance_model1_CNN_layer')\n",
    "\n",
    "num_pats = 5\n",
    "max_act_filt_num = 10\n",
    "cv_patients = create_fake_patients.create_fake_patient_df(num_pats, 99, max_event_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nans(lst):\n",
    "    cleaned_list = [x for x in lst if not math.isnan(x)]\n",
    "    return cleaned_list\n",
    "\n",
    "\n",
    "\n",
    "model2 = whole_model_demographics_gradcam.TGCNN_Model(num_filters=16, num_nodes=max_event_codes, num_time_steps=max_timesteps, \n",
    "                            filter_size=filter_size, variable_gamma=True, \n",
    "                            exponential_scaling=True, dropout_rate=0.7, lstm_units=64,\n",
    "                            fcl1_units=128, LSTM_ablation=False, stride=stride, activation_type='LeakyReLU', \n",
    "                            no_timestamp=False, second_TGCNN_layer=second_TGCNN_layer, num_labels=1)\n",
    "model2.load_weights(f'hip_1999_to_one_year_advance_model1_CNN_layer')\n",
    "\n",
    "\n",
    "weights = model2.get_weights()\n",
    "\n",
    "# add some random noise to each weight matrix\n",
    "modified_weights = []\n",
    "for w in weights:\n",
    "    w_mean = np.mean(w)\n",
    "    std_mean = np.std(w)\n",
    "    noise = np.random.normal(w_mean, std_mean, w.shape)  # mean=0, std=0.1\n",
    "    modified_w = w + noise  # Add the noise to the weights\n",
    "    modified_weights.append(modified_w)\n",
    "\n",
    "# set the modified weights back to the model\n",
    "model2.set_weights(modified_weights)\n",
    "\n",
    "model = whole_model_demographics_gradcam.TGCNN_Model(num_filters=16, num_nodes=max_event_codes, num_time_steps=max_timesteps, \n",
    "                            filter_size=filter_size, variable_gamma=True, \n",
    "                            exponential_scaling=True, dropout_rate=0.7, lstm_units=64,\n",
    "                            fcl1_units=128, LSTM_ablation=False, stride=stride, activation_type='LeakyReLU', \n",
    "                            no_timestamp=False, second_TGCNN_layer=second_TGCNN_layer, num_labels=1)\n",
    "model.load_weights(f'hip_1999_to_one_year_advance_model1_CNN_layer')\n",
    "\n",
    "\n",
    "replacement_true_lst1, max_w_filt_lst1, filt_nums1 = ga.get_act_metric_per_feat(model, num_filters, num_pats, \n",
    "                                                                                 cv_patients, max_event_codes, hip_or_knee,\n",
    "                                                                                 'max', max_timesteps)\n",
    "    \n",
    "mean_activation_df1 = ga.act_diff(replacement_true_lst1, max_w_filt_lst1, filt_nums1, show_plot=False)\n",
    "\n",
    "replacement_true_lst2, max_w_filt_lst2, filt_nums2 = ga.get_act_metric_per_feat(model2, num_filters, num_pats, \n",
    "                                                                                 cv_patients, max_event_codes, hip_or_knee,\n",
    "                                                                                 'max', max_timesteps)\n",
    "    \n",
    "mean_activation_df2 = ga.act_diff(replacement_true_lst2, max_w_filt_lst2, filt_nums2, show_plot=False)\n",
    "\n",
    "\n",
    "\n",
    "metric_type = 'largest'\n",
    "\n",
    "mse_diffs_mean, abs_diffs_mean = [], []\n",
    "mse_diffs_median, abs_diffs_median = [], []\n",
    "mse_diffs_max, abs_diffs_max = [], []\n",
    "for pat_num in range(num_pats):\n",
    "\n",
    "    if (pat_num % 50) == 0 and (pat_num !=0):\n",
    "        print(f\"{(((pat_num+1)/num_pats)*100):.2f}% Complete\")\n",
    "        mse_diffs_mean = remove_nans(mse_diffs_mean)\n",
    "        abs_diffs_mean = remove_nans(abs_diffs_mean)\n",
    "        mse_diffs_median = remove_nans(mse_diffs_median)\n",
    "        abs_diffs_median = remove_nans(abs_diffs_median)\n",
    "        mse_diffs_max = remove_nans(mse_diffs_max)\n",
    "        abs_diffs_max = remove_nans(abs_diffs_max)\n",
    "        print(f\"MSE mean +- SD: {np.mean(mse_diffs_mean):.3f} $\\pm$ {np.std(mse_diffs_mean):.3f}\")\n",
    "        print(f\"Abs mean +- SD: {np.mean(abs_diffs_mean):.3f} $\\pm$ {np.std(abs_diffs_mean):.3f}\")\n",
    "        \n",
    "        print(f\"MSE median +- SD: {np.mean(mse_diffs_median):.3f} $\\pm$ {np.std(mse_diffs_median):.3f}\")\n",
    "        print(f\"Abs median +- SD: {np.mean(abs_diffs_median):.3f} $\\pm$ {np.std(abs_diffs_median):.3f}\")\n",
    "        \n",
    "        print(f\"MSE max +- SD: {np.mean(mse_diffs_max):.3f} $\\pm$ {np.std(mse_diffs_max):.3f}\")\n",
    "        print(f\"Abs max +- SD: {np.mean(abs_diffs_max):.3f} $\\pm$ {np.std(abs_diffs_max):.3f}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Generate individual data for the model\n",
    "    input_3d, input_4d, demo_tensor, outcome, outcome_bin = utils.return_pat_from_df(cv_patients, max_event_codes, hip_or_knee, pat_num, max_timesteps)\n",
    "    \n",
    "    dense_tensor = tf.sparse.to_dense(input_3d)\n",
    "    dense_tensor= tf.transpose(dense_tensor, perm=[2, 1, 0])\n",
    "    dense_tensor = np.flip(dense_tensor, axis=0)\n",
    "\n",
    "    \n",
    "    \n",
    "    heatmap_df = pd.DataFrame()\n",
    "    # Get the entire patient's history in a DataFrame\n",
    "    edges_df = graph_plot.create_edges_df_gc(dense_tensor)\n",
    "    \n",
    "    # Get the node positions for the graph\n",
    "    pos_df = graph_plot.create_position_df_gc(edges_df)\n",
    "    pos_list = graph_plot.generate_pos_sequence(pos_df['max_codes_per_visit'].max())\n",
    "    pos_df = graph_plot.map_y_coord_to_node(pos_df, pos_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(0, num_filters):\n",
    "        weighted_f_map = ga.choose_feat_map(model, 'single', mean_activation_df1, i)\n",
    "        timestep_ave_w_df = gc.calc_timestep_weights(1, filter_size, weighted_f_map, max_timesteps)\n",
    "        read_code_pos_df = gc.map_read_code_labels(pos_df, read_code_map_df, timestep_ave_w_df)\n",
    "        # remove any rows with duplicate v number in node column\n",
    "        df_unique = read_code_pos_df.drop_duplicates(subset='x', keep=False)\n",
    "        heatmap_df[f'col_{i}'] = df_unique['perc_timestep_infl']\n",
    "    \n",
    "    # add the mean, median and max map values to the heatmap too\n",
    "    row_medians = heatmap_df.iloc[:, :].median(axis=1)\n",
    "    heatmap_df['col_32'] = row_medians\n",
    "    \n",
    "    row_means = heatmap_df.iloc[:, :-1].mean(axis=1)\n",
    "    heatmap_df['col_33'] = row_means\n",
    "    \n",
    "    weighted_f_map = ga.choose_feat_map(model, metric_type, mean_activation_df1, max_act_filt_num)\n",
    "    timestep_ave_w_df = gc.calc_timestep_weights(1, filter_size, weighted_f_map, max_timesteps)\n",
    "    read_code_pos_df = gc.map_read_code_labels(pos_df, read_code_map_df, timestep_ave_w_df)\n",
    "    df_unique = read_code_pos_df.drop_duplicates(subset='x', keep=False)\n",
    "    heatmap_df['col_34'] = df_unique['perc_timestep_infl']\n",
    "    \n",
    "    heatmap1_mean = heatmap_df['col_32'].to_numpy()\n",
    "    heatmap1_median = heatmap_df['col_33'].to_numpy()\n",
    "    heatmap1_max = heatmap_df['col_34'].to_numpy()\n",
    "    \n",
    "    # ------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    heatmap_df2 = pd.DataFrame()\n",
    "    # Get the entire patient's history in a DataFrame\n",
    "    edges_df = graph_plot.create_edges_df_gc(dense_tensor)\n",
    "    \n",
    "    # Get the node positions for the graph\n",
    "    pos_df = graph_plot.create_position_df_gc(edges_df)\n",
    "    pos_list = graph_plot.generate_pos_sequence(pos_df['max_codes_per_visit'].max())\n",
    "    pos_df = graph_plot.map_y_coord_to_node(pos_df, pos_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(0, num_filters):\n",
    "        weighted_f_map = ga.choose_feat_map(model2, 'single', mean_activation_df2, i)\n",
    "        timestep_ave_w_df = gc.calc_timestep_weights(1, filter_size, weighted_f_map, max_timesteps)\n",
    "        read_code_pos_df = gc.map_read_code_labels(pos_df, read_code_map_df, timestep_ave_w_df)\n",
    "        # remove any rows with duplicate v number in node column\n",
    "        df_unique = read_code_pos_df.drop_duplicates(subset='x', keep=False)\n",
    "        heatmap_df2[f'col_{i}'] = df_unique['perc_timestep_infl']\n",
    "    \n",
    "    # add the mean, median and max map values to the heatmap too\n",
    "    row_medians = heatmap_df2.iloc[:, :].median(axis=1)\n",
    "    heatmap_df2['col_32'] = row_medians\n",
    "    \n",
    "    row_means = heatmap_df2.iloc[:, :-1].mean(axis=1)\n",
    "    heatmap_df2['col_33'] = row_means\n",
    "    \n",
    "    weighted_f_map = ga.choose_feat_map(model2, metric_type, mean_activation_df2, max_act_filt_num)\n",
    "    timestep_ave_w_df = gc.calc_timestep_weights(1, filter_size, weighted_f_map, max_timesteps)\n",
    "    read_code_pos_df = gc.map_read_code_labels(pos_df, read_code_map_df, timestep_ave_w_df)\n",
    "    df_unique = read_code_pos_df.drop_duplicates(subset='x', keep=False)\n",
    "    heatmap_df2[f'col_{34}'] = df_unique['perc_timestep_infl']\n",
    "    \n",
    "    heatmap2_mean = heatmap_df2['col_32'].to_numpy()\n",
    "    heatmap2_median = heatmap_df2['col_33'].to_numpy()\n",
    "    heatmap2_max = heatmap_df2['col_34'].to_numpy()\n",
    "    \n",
    "    #mse_lst.append(mse(heatmap_array, heatmap_array2))\n",
    "\n",
    "    ## MEAN\n",
    "    mse_difference_mean = np.mean((heatmap1_mean - heatmap2_mean) ** 2)\n",
    "    mse_diffs_mean.append(mse_difference_mean)\n",
    "    abs_difference_mean = np.mean(np.abs(heatmap1_mean- heatmap2_mean))\n",
    "    abs_diffs_mean.append(abs_difference_mean)\n",
    "\n",
    "    ## MEDIAN\n",
    "    mse_difference_median = np.mean((heatmap1_median - heatmap2_median) ** 2)\n",
    "    mse_diffs_median.append(mse_difference_median)\n",
    "    abs_difference_median = np.mean(np.abs(heatmap1_median- heatmap2_median))\n",
    "    abs_diffs_median.append(abs_difference_median)\n",
    "\n",
    "    ## MAX\n",
    "    mse_difference_max = np.mean((heatmap1_max - heatmap2_max) ** 2)\n",
    "    mse_diffs_max.append(mse_difference_max)\n",
    "    abs_difference_max = np.mean(np.abs(heatmap1_max- heatmap2_max))\n",
    "    abs_diffs_max.append(abs_difference_max)\n",
    "    \n",
    "\n",
    "mse_diffs_mean = remove_nans(mse_diffs_mean)\n",
    "abs_diffs_mean = remove_nans(abs_diffs_mean)\n",
    "mse_diffs_median = remove_nans(mse_diffs_median)\n",
    "abs_diffs_median = remove_nans(abs_diffs_median)\n",
    "mse_diffs_max = remove_nans(mse_diffs_max)\n",
    "abs_diffs_max = remove_nans(abs_diffs_max)\n",
    "print(f\"MSE mean +- SD: {np.mean(mse_diffs_mean):.3f} $\\pm$ {np.std(mse_diffs_mean):.3f}\")\n",
    "print(f\"Abs mean +- SD: {np.mean(abs_diffs_mean):.3f} $\\pm$ {np.std(abs_diffs_mean):.3f}\")\n",
    "\n",
    "print(f\"MSE median +- SD: {np.mean(mse_diffs_median):.3f} $\\pm$ {np.std(mse_diffs_median):.3f}\")\n",
    "print(f\"Abs median +- SD: {np.mean(abs_diffs_median):.3f} $\\pm$ {np.std(abs_diffs_median):.3f}\")\n",
    "\n",
    "print(f\"MSE max +- SD: {np.mean(mse_diffs_max):.3f} $\\pm$ {np.std(mse_diffs_max):.3f}\")\n",
    "print(f\"Abs max +- SD: {np.mean(abs_diffs_max):.3f} $\\pm$ {np.std(abs_diffs_max):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_msk_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
